# -*- coding: utf-8 -*-
"""Researcher_Long_Term_Memory.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L7FHzbP_3drD-YUzzqxUtpNlgpb_9axD
"""

import os
import json
import time
import datetime
import uuid
from typing import List, Dict

# LangChain / AI Imports
from langchain_groq import ChatGroq
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain.prompts import PromptTemplate

# ---------------------------------------------------------
# CONFIGURATION
# ---------------------------------------------------------
EMBEDDER_MODEL = "BAAI/bge-base-en-v1.5"
MEMORY_FILE = "long_term_memory.json"  # Acts as our MongoDB for this demo
FAISS_INDEX_FOLDER = "faiss_memory_index"

class MemoryLayer:
    """
    Implements the Hybrid Architecture:
    1. FAISS (Vector Index): For fast Semantic Search (RAM)
    2. JSON/MongoDB (Doc Store): For persistent storage of full details (Disk)
    """
    def __init__(self, groq_api_key):
        self.groq_api_key = groq_api_key

        # 1. Initialize Embeddings (GPU if available)
        self.embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDER_MODEL,
            model_kwargs={'device': 'cpu'} # Change to 'cuda' if GPU available
        )

        # 2. Initialize LLM for "Fact Extraction" (The filter)
        self.llm = ChatGroq(
            temperature=0,
            model_name="groq/compound", # or llama3-8b-8192
            groq_api_key=self.groq_api_key
        )

        # 3. Load or Initialize Memory Stores
        self.memory_store = self._load_memory_store() # Simulates MongoDB
        self.vector_store = self._load_or_create_faiss()

    # ---------------------------------------------------------
    # INTERNAL: STORAGE MANAGEMENT
    # ---------------------------------------------------------
    def _load_memory_store(self):
        """Simulates loading from MongoDB."""
        if os.path.exists(MEMORY_FILE):
            with open(MEMORY_FILE, "r") as f:
                return json.load(f)
        return []

    def _save_memory_store(self):
        """Simulates saving to MongoDB."""
        with open(MEMORY_FILE, "w") as f:
            json.dump(self.memory_store, f, indent=2)

    def _load_or_create_faiss(self):
        """Loads FAISS index from disk or creates a new one."""
        if os.path.exists(FAISS_INDEX_FOLDER):
            return FAISS.load_local(FAISS_INDEX_FOLDER, self.embeddings, allow_dangerous_deserialization=True)
        else:
            # Create an empty index with a dummy document to initialize
            # (FAISS needs at least 1 doc to init structure)
            dummy = Document(page_content="init", metadata={"id": "init"})
            return FAISS.from_documents([dummy], self.embeddings)

    # ---------------------------------------------------------
    # CORE: EXTRACTION & STORAGE (The "Write" Path)
    # ---------------------------------------------------------
    def extract_and_save(self, user_input: str, turn_number: int):
        """
        Uses LLM to decide if this input is worth remembering.
        If yes, saves to FAISS + JSON.
        """
        # 1. Extraction Prompt
        extraction_prompt = f"""
        Analyze the User's input. Extract distinct factual preferences, constraints, or plans.
        If the input is just chit-chat (hello, thanks), return "NONE".

        User Input: "{user_input}"

        Return ONLY the factual statement (e.g., "User prefers Kannada language").
        """

        # 2. Fast Inference (Fact Extraction)
        # Note: We use invoke/call structure depending on langchain version
        fact = self.llm.invoke(extraction_prompt).content.strip()

        if "NONE" in fact or len(fact) < 5:
            return None # Don't memorize noise

        # 3. Create Memory Object
        memory_id = str(uuid.uuid4())
        memory_obj = {
            "id": memory_id,
            "content": fact,
            "original_text": user_input,
            "turn": turn_number,
            "timestamp": str(datetime.datetime.now())
        }

        # 4. Save to JSON (MongoDB)
        self.memory_store.append(memory_obj)
        self._save_memory_store()

        # 5. Save to FAISS (Vector Index)
        doc = Document(page_content=fact, metadata={"id": memory_id, "turn": turn_number})
        self.vector_store.add_documents([doc])
        self.vector_store.save_local(FAISS_INDEX_FOLDER)

        return fact

    # ---------------------------------------------------------
    # CORE: RETRIEVAL (The "Read" Path)
    # ---------------------------------------------------------
    def retrieve_relevant_memories(self, query: str, k=3):
        """
        1. Embeds the query.
        2. Searches FAISS for semantic matches.
        3. Returns the text to feed into the main Chatbot.
        """
        # Search FAISS
        # We exclude the "init" dummy doc if it comes up
        results = self.vector_store.similarity_search(query, k=k+1)

        memories = []
        for res in results:
            if res.metadata.get("id") == "init": continue

            # Formulate the context string
            # In a real DB, we would use res.metadata['id'] to fetch more fields from Mongo
            memories.append(f"[Memory from Turn {res.metadata.get('turn')}]: {res.page_content}")

        return "\n".join(memories[:k])

# ---------------------------------------------------------
# MAIN BOT INTEGRATION
# ---------------------------------------------------------
class LongTermMemoryBot:
    def __init__(self):
        self.groq_api_key = os.getenv("GROQ_API_KEY", "your_key_here")
        self.memory_layer = MemoryLayer(self.groq_api_key)
        self.llm = ChatGroq(temperature=0.7, model_name="groq/compound", groq_api_key=self.groq_api_key)
        self.turn_counter = 0

    def chat(self, user_input):
        self.turn_counter += 1

        # STEP 1: RETRIEVAL (Latency: ~50ms)
        past_context = self.memory_layer.retrieve_relevant_memories(user_input)

        # STEP 2: AUGMENT PROMPT
        system_prompt = f"""
        You are a helpful assistant with long-term memory.

        RELEVANT PAST MEMORIES:
        {past_context if past_context else "No relevant past memories."}

        CURRENT USER INPUT:
        {user_input}

        Answer the user naturally, using the past memories if they apply.
        """

        # STEP 3: GENERATE ANSWER (Latency: Normal LLM time)
        response = self.llm.invoke(system_prompt).content

        # STEP 4: EXTRACTION (Async / Background Process)
        # We run this *after* or parallel to generation so the user doesn't wait
        saved_fact = self.memory_layer.extract_and_save(user_input, self.turn_counter)

        print(f"\n--- Debug Info ---")
        print(f"Retrieved: {past_context}")
        print(f"New Memory Saved: {saved_fact}")
        print(f"------------------\n")

        return response

# ---------------------------------------------------------
# SIMULATION
# ---------------------------------------------------------
if __name__ == "__main__":
    bot = LongTermMemoryBot()

    # Simulate Turn 1
    print("User: My favorite food is Pizza.")
    print("Bot:", bot.chat("My favorite food is Pizza."))

    print("\n... (1000 turns later) ...\n")

    # Simulate Turn 1000
    print("User: What should I order for dinner?")
    print("Bot:", bot.chat("What should I order for dinner?"))